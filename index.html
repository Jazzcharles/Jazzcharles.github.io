<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jilan Xu</title>
  
  <meta name="author" content="Jilan Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jilan Xu</name>
		<br>
		 <a href="mailto:[deletethis] jilanxu18@fudan.edu.cn", style="color:grey">jilanxu18 at fudan dot edu dot cn</a>
		<br>
		<br>
              <font size=4>
              <p>I am a final year PhD student at Fudan University, advised by Professor <a href="https://cs.fudan.edu.cn/3e/d0/c25921a278224/page.htm###"><font size="4">Yuejie Zhang</font></a>. I also work closely with Professor <a href="https://weidixie.github.io/"><font size="4">Weidi Xie</font></a>. My research focuses on multimodal machine learning, video understanding, and medical image analysis. I hope that someday medical AI agents would heal the world, make it a better place, for the entire human race.
                <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=mf2U64IAAAAJ&hl=en&oi=ao"><font size=4>Google Scholar</font></a> &nbsp/&nbsp
		            <a href="https://twitter.com/JazzzCharles"><font size=4>Twitter</font></a> &nbsp/&nbsp
                <a href="https://github.com/Jazzcharles"><font size=4>GitHub</font> </a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/Jazz_charles"><font size=4>Zhihu</font></a>&nbsp
              </p>
            </font>

            </td>
            <style>
              
            </style> 
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/icon2_square.jpeg"><img style="width:70%;max-width:70%;border-radius:50%" alt="profile photo" src="images/icon2_square.jpeg" class="img-circle" ></a>
            </td>
          </tr>
        </tbody></table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="30"><tbody>
          <tr>
            <td>
              <heading><font color="red">News</font></heading>
              <font size=4>
              <p>
                  <ul style="line-height: 1.8;">
                    <li style="margin-bottom: 12px;">
                      <strong><font size=5 color="red">I'm actively looking for postdoc/job positions in 2025, please feel free to email me!</font></strong>
                    </li>
                    <li style="margin-bottom: 12px;">
                      [01/2025] Three Papers (<strong><font size=4>XGen</font></strong>, <strong><font size=4>EgoVideo</font></strong>, <strong><font size=4>CGBench</font></strong>) accepted to ICLR 2025!!!
                    </li>
                    <li style="margin-bottom: 12px;">
                      [01/2025] Honored to be invited to give a talk on joint egocentric-exocentric video understanding at <a href="https://www.techbeat.net/talk-info?id=940"><font size=4>TechBeat</font></a>
                    </li>
                    <li style="margin-bottom: 12px;">
                      [05/2024] Our CVPR papers <strong><font size=4>Egoinstructor</font></strong> and <strong><font size=4>EgoExoLearn</font></strong> are also accepted to 1st LPVL Workshop @ CVPR 2024
                    </li>
                    <li style="margin-bottom: 12px;">
                      [04/2024] We rank <strong>1st</strong> at 4th-COV19D Competition Track 2 and 4th at Track1 @ CVPR 2024
                    </li>
                  </ul>
              </p>
            </font>
            </td>
          </tr> 
        </tbody></table>



        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>

          </td>
            </tr>
        </tbody></table>
        <style>
          img {
            border-radius: 15px;
          }
        </style> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        
        <div class="tab">
          <button class="tablinks active" onclick="openTab(event, 'ComputerVision')">Computer Vision</button>
          <button class="tablinks" onclick="openTab(event, 'Medical')">Medical Image Analysis</button>
        </div>
        
        <div id="ComputerVision" class="tabcontent" style="display:block;">
          <!-- 将Computer Vision相关的研究项目放在这里 -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- 这里放置Computer Vision相关的项目 -->
            <tr></tr>
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/xgen_teaser_square.png" alt="xgen" width="250" height="250">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org"> 
                  <papertitle style="font-size: 20px";> XGen: Egocentric Video Prediction by Watching Exocentric Videos </papertitle>
                </a>
                <br>
                    <b>Jilan Xu</b>, Yifei Huang, Baoqi Pei, Junlin Hou, Qingqiu Li, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                <br>
                <em>ICLR</em> 2025 &nbsp 
                <br>
                <!-- <a href="https://arxiv.org"> <font size=4>arXiv</font></a> / <a href="https://egoexolearn.github.io/"><font size=4>project page</font></a> / <a href="https://github.com/OpenGVLab/EgoExoLearn"><font size=4>code</font></a> -->
                <p></p>
                <p>
                  <font size=4>
                    A cross-view video prediction model that predicts future egocentric video frames by leveraging paired exocentric video and text instructions. 
                  </font> 
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/egovideo.png" alt="egovideo" width="280" height="240">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org"> 
                  <papertitle style="font-size: 20px";> Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning </papertitle>
                </a>
                <br>
                    Baoqi Pei, Yifei Huang, <b>Jilan Xu</b>, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang
                <br>
                <em>ICLR</em> 2025 &nbsp 
                <br>
                <!-- <a href="https://arxiv.org/pdf/2403.16182.pdf"> <font size=4>arXiv</font></a> / <a href="https://egoexolearn.github.io/"><font size=4>project page</font></a> / <a href="https://github.com/OpenGVLab/EgoExoLearn"><font size=4>code</font></a> -->
                <p></p>
                <p>
                  <font size=4>
                    An egocentric video-language model that learns fine-grained egocentric video representations by modeling hand-object dynamics. 
                  </font> 
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/egoexolearn.png" alt="egoexolearn" width="280" height="220">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2403.16182"> 
                  <papertitle style="font-size: 20px";> EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World </papertitle>
                </a>
                <br>
                    Yifei Huang*, Guo Chen*, <b>Jilan Xu*</b>, Mingfang Zhang*, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, Yu Qiao,
                <br>
                <em>CVPR</em> 2024 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2403.16182.pdf"> <font size=4>arXiv</font></a> / <a href="https://egoexolearn.github.io/"><font size=4>project page</font></a> / <a href="https://github.com/OpenGVLab/EgoExoLearn"><font size=4>code</font></a>
                <p></p>
                <p>
                  <font size=4>
                    A cross-view benchmark dataset that emulates the human demonstration following process, containing recorded egocentric videos guided by exocentric-view demonstration videos. 
                  </font> 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/egoinstructor.png" alt="egoinstructor" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2401.00789"> 
                  <papertitle style="font-size: 20px";> Retrieval-Augmented Egocentric Video Captioning </papertitle>
                </a>
                <br>
                    <b>Jilan Xu</b>, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                <br>
                <em>CVPR</em> 2024 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2401.00789.pdf"> <font size=4>arXiv</font></a> / <a href="https://jazzcharles.github.io/Egoinstructor"><font size=4>project page</font></a> / <a href="https://github.com/Jazzcharles/Egoinstructor"><font size=4>code</font></a>
                <p></p>
                <p>
                  <font size=4>
                    Given an egocentric video, Egoinstructor automatically retrieves relevant exocentric instructional videos for assisting egocentric video captioning.
                  </font> 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ovsegmentor.png" alt="ovsegmentor" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2301.09121">
                  <papertitle style="font-size: 20px";> Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision </papertitle>
                </a>
                <br>
                    <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, Weidi Xie
                <br>
                <em>CVPR</em> 2023 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2301.09121.pdf"><font size=4>arXiv</font></a> / <a href="https://jazzcharles.github.io/OVSegmentor"><font size=4>project page</font></a> / <a href="https://github.com/Jazzcharles/OVSegmentor"><font size=4>code</font></a>
                <p></p>
                <p>
                  <font size=4>
                    Training open-vocabulary semantic segmentation models with image-text pairs only, which enables zero-transfer to various segmentation datasets. 
                  </font>
                </p>
                </td>
            </tr>
  

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cream.png" alt="cream" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2205.13922">
                  <papertitle style="font-size: 20px";> CREAM: Weakly supervised object localization via class re-activation mapping </papertitle>
                </a>
                <br>
                    <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Rui-Wei Zhao, Tao Zhang, Xuequan Lu, Shang Gao
                <br>
                <em>CVPR</em> 2022 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2205.13922.pdf"><font size=4>arXiv</font></a>
                <p></p>
                <p>
                  <font size=4>
                  A weakly-supervised object localization model that generates better CAMs via soft-clustering algorithms. 
                  </font>
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ovoad.png" alt="cream" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://openreview.net/forum?id=PWzB2V2b6R">
                  <papertitle style="font-size: 20px";> Does video-text pretraining help open vocabulary online action detection </papertitle>
                </a>
                <br>
                    Qingsong Zhao, Yi Wang, <b>Jilan Xu</b>, Yinan He, Zifan Song, Limin Wang, Yu Qiao, Cairong Zhao
                <br>
                <em>NeurIPS</em> 2024 &nbsp 
                <br>
                <a href="https://openreview.net/forum?id=PWzB2V2b6R"><font size=4>arXiv</font></a>
                <p></p>
                <p>
                  <font size=4>
                    A zero-shot online action detector that leverages vision-language models and enables open-world temporal understanding.
                  </font>
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/internvideo.png" alt="internvideo" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2212.03191">
                  <papertitle style="font-size: 20px";> InternVideo: General Video Foundation Models via Generative and Discriminative Learning </papertitle>
                </a>
                <br>
                  Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, <b>Jilan Xu</b>, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao
                <br>
                <em>Tech report</em> 2022 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2212.03191.pdf"><font size=4>arXiv</font></a> / <a href="https://github.com/OpenGVLab/InternVideo"><font size=4>code</font></a>
                <p></p>
                <p>
                  <font size=4>
                    A fundation model for video / video-text understanding, achieving SOTA over 30 benchmark datasets. 
                  </font>
                </p>
              </td>
            </tr>


            

          </tbody></table>
        </div>
        
        <div id="Medical" class="tabcontent">
          <!-- 将Medical相关的研究项目放在这里 -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- 这里放置Medical相关的项目 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/caw.png" alt="caw" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2404.05997">
                  <papertitle style="font-size: 20px";> Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis           </papertitle>
                </a>
                <br>
                    Junlin Hou, <b>Jilan Xu</b>, Hao Chen
                <br>
                <em>MICCAI</em> 2024 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2404.05997"><font size=4>arXiv</font></a> 
                <p></p>
                <p>
                  <font size=4>
                    An XAI framework that aligns the axes of the latent space with concepts of interest for interpretable skin lesion diagnosis.
                  </font>
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/anatomy.png" alt="caw" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2403.09294">
                  <papertitle style="font-size: 20px";>   Anatomical structure-guided medical vision-language pre-training                  </papertitle>
                </a>
                <br>
                  Qingqiu Li, Xiaohan Yan, <b>Jilan Xu</b>, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Shujun Wang
                <br>
                <em>MICCAI</em> 2024 &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2403.09294"><font size=4>arXiv</font></a> 
                <p></p>
                <p>
                  <font size=4>
                      An Anatomical Structure-Guided visual-text pre-training framework that leverages the anatomical knowledge.
                  </font>
                </p>
              </td>
            </tr>
            

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cmcv2.png" alt="cmcv2" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://arxiv.org/abs/2211.14557">
                  <papertitle style="font-size: 20px";> CMC_v2: Towards More Accurate COVID-19 Detection with Discriminative Video Priors </papertitle>
                </a>
                <br>
                    Junlin Hou, <b>Jilan Xu</b>, Nan Zhang, Yi Wang, Yuejie Zhang, Xiaobo Zhang, Rui Feng
                <br>
                <em>ECCV</em> 2022 AIMIA Workshop &nbsp 
                <br>
                <a href="https://arxiv.org/pdf/2211.14557.pdf"><font size=4>arXiv</font></a> / <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution"><font size=4>code</font></a>
                <p></p>
                <p>
                  <font size=4>
                    A Transformer-based model with contrastive representation enhancement. Winner of the 2nd COVID-19 Detection in ECCV 2022. 
                  </font>
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/tccnet.png" alt="tccnet" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://www.ijcai.org/proceedings/2022/0155.pdf">
                  <papertitle style="font-size: 20px";> TCCNet: Temporally Consistent Context-Free Network for Semi-supervised Video Polyp Segmentation </papertitle>
                </a>
                <br>
                  Xiaotong Li, <b>Jilan Xu</b>, Yuejie Zhang, Rui Feng, Rui-Wei Zhao, Tao Zhang, Xuequan Lu, Shang Gao
                <br>
                <em>IJCAI</em> 2022, Oral &nbsp 
                <br>
                <a href="https://www.ijcai.org/proceedings/2022/0155.pdf"><font size=4>paper</font></a>
                <p></p>
                <p>
                  <font size=4>
                    Co-training a model for semi-supervised video polyp segmentation, achieving comparable results using only 15% labeled data.
                  </font>
                </p>
                  </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cmcv1.png" alt="cmcv1" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf">
                  <papertitle style="font-size: 20px";> CMC-COV19D: Contrastive Mixup Classification for COVID-19 Diagnosis </papertitle>
                </a>
                <br>
                  Junlin Hou*, <b>Jilan Xu*</b>, Rui Feng, Yuejie Zhang, Fei Shan, Weiya Shi
                <br>
                <em>ICCV</em> 2021, AIMIA Workshop. &nbsp 
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf"><font size=4>paper</font></a> / <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution"><font size=4>code</font></a>
                <p></p>
                <p>
                  <font size=4>
                    A ResNest-50 model combined with contrastive mixup technique for 3D COVID-19 CT image classification. Winner of the 1st COVID-19 detection challenge.
                  </font>
                </p>
              </td>
            </tr> 
 
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/drl.png" alt="drl" width="280" height="200">
              </td>
              <td width="75%" valign="middle" style="font-size: 18px;">
                <a href="https://ieeexplore.ieee.org/document/9313159/#:~:text=Data-Efficient%20Histopathology%20Image%20Analysis%20with%20Deformation%20Representation%20Learning,are%20often%20expensive%20and%20laborious%20in%20realworld%20scenarios.">
                  <papertitle style="font-size: 20px";> Data-Efficient Histopathology Image Analysis with Deformation Representation Learning </papertitle>
                </a>
                <br>
                  <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Chunyang Ruan, Tao Zhang, Weiguo Fan
                <br>
                <em>BIBM</em> 2020, Oral &nbsp 
                <br>
                <a href="https://ieeexplore.ieee.org/document/9313159/#:~:text=Data-Efficient%20Histopathology%20Image%20Analysis%20with%20Deformation%20Representation%20Learning,are%20often%20expensive%20and%20laborious%20in%20realworld%20scenarios."><font size=4>paper</font></a>
                <p></p>
                <p>
                  <font size=4>
                    Introducing a self-supervised deformation representation learning technique for histopathology image analysis.
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
        </div>
        
        <style>
          @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');
        
          img {
            border-radius: 15px;
          }
          .tab {
            overflow: hidden;
            background-color: #f8f8f8;
            font-family: 'Roboto', sans-serif;
          }
          .tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 18px;
            font-weight: bold;
            color: #333;
          }
          .tab button:hover {
            background-color: #ddd;
          }
          .tab button.active {
            background-color: #fff;
            border-bottom: 3px solid #4CAF50;
          }
          .tabcontent {
            display: none;
            padding: 20px 0;
            border: none;
          }
          .tabcontent table {
            border: none;
          }
        </style>
        
        <script>
        function openTab(evt, tabName) {
          var i, tabcontent, tablinks;
          tabcontent = document.getElementsByClassName("tabcontent");
          for (i = 0; i < tabcontent.length; i++) {
            tabcontent[i].style.display = "none";
          }
          tablinks = document.getElementsByClassName("tablinks");
          for (i = 0; i < tablinks.length; i++) {
            tablinks[i].className = tablinks[i].className.replace(" active", "");
          }
          document.getElementById(tabName).style.display = "block";
          evt.currentTarget.className += " active";
        }
        </script>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards & Honors</heading>
              <font size=4>
              <p>
                  <ul>
                    <li>Winner of the 4th-COV19D Competition Track 2 (COVID19 Domain Adaptation Challenge) and rank 4th at Track1 (COVID-19 Detection Challenge) @ CVPR 2024</li>
                    <li>Winner of the MMAC Challenge Track1 (Classification of Myopic Maculopathy) and Track2 (Segmentation of Myopic Maculopathy Plus Lesions) @ MICCAI 2023</li>
                    <li>Winner of the 1st & 2nd COVID-19 Detection Challenge @ ICCV 2021 & ECCV 2022</li>
                    <li>Winner of the 1st COVID-19 Severity Detection Challenge @ ECCV 2022</li>
                    <li>VenusTech Enterprise Scholarship</li>
                  </ul>
              </p>
            </font>
            </td>
          </tr> 
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Working Experience</heading>
              <div class="experience-container">
                <div class="experience-item">
                  <img src="images/shanghai-ai-lab-logo.png" class="company-logo" alt="Shanghai AI Laboratory Logo">
                  <div class="experience-content">
                    <div class="company">Shanghai AI Laboratory</div>
                    <div class="role">Research Intern</div>
                    <div class="supervisor">Supervised by Dr. Yifei Huang, Yi Wang and Prof. Yu Qiao</div>
                  </div>
                </div>

                <div class="experience-item">
                  <img src="images/bell-ai-logo.png" class="company-logo" alt="Bell AI Lab Logo">
                  <div class="experience-content">
                    <div class="company">Bell AI Lab, Shanghai</div> 
                    <div class="role">Research Intern</div>
                    <div class="supervisor">Supervised by Dr. Chenhui Ye</div>
                  </div>
                </div>

                <div class="experience-item">
                  <img src="images/google-logo.png" class="company-logo" alt="Google Logo">
                  <div class="experience-content">
                    <div class="company">Google Winter AI Camp</div>
                    <div class="achievement">🏆 Best Presentation Award Winner</div>
                  </div>
                </div>

                <div class="experience-item">
                  <img src="images/morgan-stanley-logo.png" class="company-logo" alt="Morgan Stanley Logo">
                  <div class="experience-content">
                    <div class="company">Morgan Stanley Technology</div>
                    <div class="role">Software Engineering Intern</div>
                    <div class="supervisor">Supervised by Ray Zhou</div>
                  </div>
                </div>
              </div>

              <style>
                .experience-container {
                  font-size: 16px;
                  line-height: 1.8;
                }
                .experience-item {
                  margin-bottom: 15px;
                  padding: 5px;
                  border-left: 3px solid #007bff;
                  background: #f8f9fa;
                  display: flex;
                  align-items: center;
                  gap: 20px;
                }
                .company-logo {
                  width: 100px;
                  height: 100px;
                  object-fit: contain;
                  flex-shrink: 0;
                }
                .experience-content {
                  flex-grow: 1;
                }
                .company {
                  font-weight: bold;
                  color: #2c3e50;
                }
                .role {
                  color: #666;
                  margin-top: 3px;
                }
                .supervisor {
                  color: #666;
                  font-style: italic;
                  margin-top: 3px;
                }
                .achievement {
                  color: #28a745;
                  margin-top: 3px;
                }
              </style>
            </td>
          </tr>
        </tbody></table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
              <p>
                <font size=4>
                <strong> <font size=4>Conference Reviewer</font> </strong>: ICLR25, NeurIPS24, ECCV24, MICCAI24, CVPR24, CVPR23, ICCV23, NeurIPS22
                </font>
              </p>
              <p>
                <font size=4>
                <strong> <font size=4>Journal Reviewer</font> </strong>: Nature Communications, TPAMI, IJCV, TMM, NeuroComputing
                </font>
              </p>

              <p>
                <font size=4>
                <strong> <font size=4>TA </font></strong>: Data Structure, The Theory of Computation
                </font>
              </p>
            </td>
          </tr> 
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
