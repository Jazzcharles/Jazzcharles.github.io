<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jilan Xu</title>
  
  <meta name="author" content="Jilan Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jilan Xu</name>
		<br>
		 <a href="mailto:[deletethis] jilanxu18@fudan.edu.cn", style="color:grey">jilanxu18 at fudan dot edu dot cn</a>
		<br>
		<br>
              <font size=4>
              <p>I am a third year PhD student at Fudan University, advised by Professor <a href="https://cs.fudan.edu.cn/3e/d0/c25921a278224/page.htm###"><font size="4">Yuejie Zhang</font></a>. I also work closely with Professor <a href="https://weidixie.github.io/"><font size="4">Weidi Xie</font></a>. My research focuses on multimodal machine learning, video understanding, and medical image analysis. I hope that someday medical AI agents would heal the world, make it a better place, for the entire human race.
                <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=mf2U64IAAAAJ&hl=en&oi=ao"><font size=4>Google Scholar</font></a> &nbsp/&nbsp
		            <a href="https://twitter.com/JazzzCharles"><font size=4>Twitter</font></a> &nbsp/&nbsp
                <a href="https://github.com/Jazzcharles"><font size=4>GitHub</font> </a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/Jazz_charles"><font size=4>Zhihu</font></a>&nbsp
              </p>
            </font>

            </td>
            <style>
              
            </style> 
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/icon2_square.jpeg"><img style="width:70%;max-width:70%;border-radius:50%" alt="profile photo" src="images/icon2_square.jpeg" class="img-circle" ></a>
            </td>
          </tr>
        </tbody></table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading><font color="red">News</font></heading>
              <font size=4>
              <p>
                  <ul>
                    <li>[05/2024] Our CVPR papers <strong>Egoinstructor</strong> and <strong>EgoExoLearn</strong> are also accepted to 1st LPVL Workshop @ CVPR 2024</li>
                    <li>[04/2024] We rank <strong>1st</strong> at 4th-COV19D Competition Track 2 and 4th at Track1 @ CVPR 2024</li>
                  </ul>
              </p>
            </font>
            </td>
          </tr> 
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>

          </td>
            </tr>
        </tbody></table>
        <style>
          img {
            border-radius: 15px;
          }
        </style>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              
              <tr>
                <td style="padding:20px;width:10%;vertical-align:middle;">
                  <img src="images/egoexolearn.png" alt="egoexolearn" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://arxiv.org/abs/2403.16182"> 
                    <papertitle style="font-size: 20px";> EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World </papertitle>
                  </a>
                  <br>
                      Yifei Huang*, Guo Chen*, <b>Jilan Xu*</b>, Mingfang Zhang*, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, Yu Qiao,
                  <br>
                  <em>CVPR</em> 2024 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2403.16182.pdf"> <font size=4>arXiv</font></a> / <a href="https://egoexolearn.github.io/"><font size=4>project page</font></a> / <a href="https://github.com/OpenGVLab/EgoExoLearn"><font size=4>code</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      A cross-view benchmark dataset that emulates the human demonstration following process, containing recorded egocentric videos guided by exocentric-view demonstration videos. 
                    </font> 
                  </p>
                </td>
              </tr>
                        
              <tr>
                <td style="padding:20px;width:10%;vertical-align:middle;">
                  <img src="images/egoinstructor.png" alt="egoinstructor" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://arxiv.org/abs/2401.00789"> 
                    <papertitle style="font-size: 20px";> Retrieval-Augmented Egocentric Video Captioning </papertitle>
                  </a>
                  <br>
                      <b>Jilan Xu</b>, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                  <br>
                  <em>CVPR</em> 2024 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2401.00789.pdf"> <font size=4>arXiv</font></a> / <a href="https://jazzcharles.github.io/Egoinstructor"><font size=4>project page</font></a> / <a href="https://github.com/Jazzcharles/Egoinstructor"><font size=4>code</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      Given an egocentric video, Egoinstructor automatically retrieves relevant exocentric instructional videos for assisting egocentric video captioning.
                    </font> 
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ovsegmentor.png" alt="ovsegmentor" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://arxiv.org/abs/2301.09121">
                    <papertitle style="font-size: 20px";> Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision </papertitle>
                  </a>
                  <br>
			                <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, Weidi Xie
                  <br>
                  <em>CVPR</em> 2023 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2301.09121.pdf"><font size=4>arXiv</font></a> / <a href="https://jazzcharles.github.io/OVSegmentor"><font size=4>project page</font></a> / <a href="https://github.com/Jazzcharles/OVSegmentor"><font size=4>code</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      Training open-vocabulary semantic segmentation models with image-text pairs only, which enables zero-transfer to various segmentation datasets. 
                    </font>
                  </p>
                  </td>
              </tr>
		

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cream.png" alt="cream" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://arxiv.org/abs/2205.13922">
                    <papertitle style="font-size: 20px";> CREAM: Weakly supervised object localization via class re-activation mapping </papertitle>
                  </a>
                  <br>
			                <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Rui-Wei Zhao, Tao Zhang, Xuequan Lu, Shang Gao
                  <br>
                  <em>CVPR</em> 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2205.13922.pdf"><font size=4>arXiv</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                    A weakly-supervised object localization model that generates better CAMs via soft-clustering algorithms. 
                    </font>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cmcv2.png" alt="cmcv2" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://arxiv.org/abs/2211.14557">
                    <papertitle style="font-size: 20px";> CMC_v2: Towards More Accurate COVID-19 Detection with Discriminative Video Priors </papertitle>
                  </a>
                  <br>
                      Junlin Hou, <b>Jilan Xu</b>, Nan Zhang, Yi Wang, Yuejie Zhang, Xiaobo Zhang, Rui Feng
                  <br>
                  <em>ECCV</em> 2022 AIMIA Workshop &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2211.14557.pdf"><font size=4>arXiv</font></a> / <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution"><font size=4>code</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      A Transformer-based model with contrastive representation enhancement. Winner of the 2nd COVID-19 Detection in ECCV 2022. 
                    </font>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/internvideo.png" alt="internvideo" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://arxiv.org/abs/2212.03191">
                    <papertitle style="font-size: 20px";> InternVideo: General Video Foundation Models via Generative and Discriminative Learning </papertitle>
                  </a>
                  <br>
                    Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, <b>Jilan Xu</b>, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao
                  <br>
                  <em>Tech report</em> 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2212.03191.pdf"><font size=4>arXiv</font></a> / <a href="https://github.com/OpenGVLab/InternVideo"><font size=4>code</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      A fundation model for video / video-text understanding, achieving SOTA over 30 benchmark datasets. 
                    </font>
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/tccnet.png" alt="tccnet" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://www.ijcai.org/proceedings/2022/0155.pdf">
                    <papertitle style="font-size: 20px";> TCCNet: Temporally Consistent Context-Free Network for Semi-supervised Video Polyp Segmentation </papertitle>
                  </a>
                  <br>
                    Xiaotong Li, <b>Jilan Xu</b>, Yuejie Zhang, Rui Feng, Rui-Wei Zhao, Tao Zhang, Xuequan Lu, Shang Gao
                  <br>
                  <em>IJCAI</em> 2022, Oral &nbsp 
                  <br>
                  <a href="https://www.ijcai.org/proceedings/2022/0155.pdf"><font size=4>paper</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      Co-training a model for semi-supervised video polyp segmentation, achieving comparable results using only 15% labeled data.
                    </font>
                  </p>
                    </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cmcv1.png" alt="cmcv1" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf">
                    <papertitle style="font-size: 20px";> CMC-COV19D: Contrastive Mixup Classification for COVID-19 Diagnosis </papertitle>
                  </a>
                  <br>
                    Junlin Hou*, <b>Jilan Xu*</b>, Rui Feng, Yuejie Zhang, Fei Shan, Weiya Shi
                  <br>
                  <em>ICCV</em> 2021, AIMIA Workshop. &nbsp 
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf"><font size=4>paper</font></a> / <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution"><font size=4>code</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      A ResNest-50 model combined with contrastive mixup technique for 3D COVID-19 CT image classification. Winner of the 1st COVID-19 detection challenge.
                    </font>
                  </p>
                </td>
              </tr> 

              
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/drl.png" alt="drl" width="280" height="200">
                </td>
                <td width="75%" valign="middle" style="font-size: 18px;">
                  <a href="https://ieeexplore.ieee.org/document/9313159/#:~:text=Data-Efficient%20Histopathology%20Image%20Analysis%20with%20Deformation%20Representation%20Learning,are%20often%20expensive%20and%20laborious%20in%20realworld%20scenarios.">
                    <papertitle style="font-size: 20px";> Data-Efficient Histopathology Image Analysis with Deformation Representation Learning </papertitle>
                  </a>
                  <br>
                    <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Chunyang Ruan, Tao Zhang, Weiguo Fan
                  <br>
                  <em>BIBM</em> 2020, Oral &nbsp 
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9313159/#:~:text=Data-Efficient%20Histopathology%20Image%20Analysis%20with%20Deformation%20Representation%20Learning,are%20often%20expensive%20and%20laborious%20in%20realworld%20scenarios."><font size=4>paper</font></a>
                  <p></p>
                  <p>
                    <font size=4>
                      Introducing a self-supervised deformation representation learning technique for histopathology image analysis.
                    </font>
                  </p>
                </td>
              </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards & Honors</heading>
              <font size=4>
              <p>
                  <ul>
                    <li>Winner of the 4th-COV19D Competition Track 2 (COVID19 Domain Adaptation Challenge) and rank 4th at Track1 (COVID-19 Detection Challenge) @ CVPR 2024</li>
                    <li>Winner of the MMAC Challenge Track1 (Classification of Myopic Maculopathy) and Track2 (Segmentation of Myopic Maculopathy Plus Lesions) @ MICCAI 2023</li>
                    <li>Winner of the 1st & 2nd COVID-19 Detection Challenge @ ICCV 2021 & ECCV 2022</li>
                    <li>Winner of the 1st COVID-19 Severity Detection Challenge @ ECCV 2022</li>
                    <li>VenusTech Enterprise Scholarship</li>
                  </ul>
              </p>
            </font>
            </td>
          </tr> 
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Working Experience</heading>
              <font size=4>
              <p>
                  <ul>
                    <li>Research intern @ Shanghai AI Laboratory, supervised by Dr.Yifei Huang, Yi Wang and Prof. Yu Qiao.</li>
                    <li>Research intern @ Bell AI Lab, Shanghai, supervised by Dr. Chenhui Ye.</li>
                    <li>Google Winter AI Camp. Our team won the best presentation award !!!</li>
                    <li>SWE intern @ Morgan Stanley Technology, supervised by Ray Zhou.</li>
                  </ul>
              </p>
              </font>
            </td>
          </tr> 
        </tbody></table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
              <p>
                <font size=4>
                <strong> <font size=4>Reviewer</font> </strong>: ECCV24, MICCAI24, CVPR24, CVPR23, ICCV23, NeurIPS22, IJCV, TMM, NeuroComputing
                </font>
              </p>

              <p>
                <font size=4>
                <strong> <font size=4>TA </font></strong>: Data Structure, Theory of Computation
                </font>
              </p>
            </td>
          </tr> 
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
