<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jilan Xu</title>
  
  <meta name="author" content="Jilan Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jilan Xu</name>
		<br>
		 <a href="mailto:[deletethis] jilanxu18@fudan.edu.cn", style="color:grey">jilanxu18 at fudan dot edu dot cn</a>
		<br>
		<br>
            <!-- <font size=4>
              <p>I am a final year PhD student at Fudan University, advised by Professor <a href="https://cs.fudan.edu.cn/3e/d0/c25921a278224/page.htm###"><font size="4">Yuejie Zhang</font></a>. I also work closely with Professor <a href="https://weidixie.github.io/"><font size="4">Weidi Xie</font></a>. My research focuses on <strong><font size=4> multimodal video understanding </font></strong>, <strong><font size=4> visual representation learning </font></strong> and <strong><font size=4> medical image analysis </font></strong>. I hope that someday medical AI agents would heal the world, make it a better place, for the entire human race.
                <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=mf2U64IAAAAJ&hl=en&oi=ao"><font size=4>Google Scholar</font></a> &nbsp/&nbsp
		            <a href="https://twitter.com/JazzzCharles"><font size=4>Twitter</font></a> &nbsp/&nbsp
                <a href="https://github.com/Jazzcharles"><font size=4>GitHub</font> </a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/Jazz_charles"><font size=4>Zhihu</font></a>&nbsp
              </p>
            </font> -->
            <font size=4>
              <p style="text-align: justify;">
                I am a final year PhD student at Fudan University, advised by Professor <a href="https://cs.fudan.edu.cn/3e/d0/c25921a278224/page.htm###"><font size="4">Yuejie Zhang</font></a>. I also work closely with Professor <a href="https://weidixie.github.io/"><font size="4">Weidi Xie</font></a>. My research focuses on <strong><font size="4">multimodal video understanding</font></strong>, <strong><font size="4">visual representation learning</font></strong> and <strong><font size="4">medical image analysis</font></strong>. I hope that someday medical AI agents would heal the world, make it a better place, for the entire human race.
              </p>
              <p style="text-align: center;">
                <a href="https://scholar.google.com/citations?user=mf2U64IAAAAJ&hl=en&oi=ao"><font size="4">Google Scholar</font></a> &nbsp/&nbsp
                <a href="https://twitter.com/JazzzCharles"><font size="4">Twitter</font></a> &nbsp/&nbsp
                <a href="https://github.com/Jazzcharles"><font size="4">GitHub</font></a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/Jazz_charles"><font size="4">Zhihu</font></a>
              </p>
            </font>

            </td>
            <style>
              
            </style> 
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/icon2_square.jpeg"><img style="width:70%;max-width:70%;border-radius:50%" alt="profile photo" src="images/icon2_square.jpeg" class="img-circle" ></a>
            </td>
          </tr>
        </tbody></table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <h2 style="font-family: Arial, Helvetica, sans-serif; font-size: 26px; font-weight: 700; color: #d9534f; margin-bottom: 20px;">
                  📢 News
                </h2>
        
                <ul class="news-list">
                  <li>
                    <span class="news-date">[09/2025]</span> One paper&nbsp;<strong><font size=4>VinCi </font></strong>&nbsp;is accepted to&nbsp;<em><font size=4>IMWUT 2025</font></em>&nbsp;!
                  </li>
                  <li>
                    <span class="news-date">[09/2025]</span> We rank&nbsp;<strong><font size=4>1st</font></strong>&nbsp;at (1) Fair Disease Challenge and (2) Multi-Source COVID19 Detection Challenge @ <em><font size=4>ICCV 2025</font></em>
                  </li>
                  <li>
                    <span class="news-date">[07/2025]</span> One paper&nbsp;<strong><font size=4>Streamformer</font></strong>&nbsp;is accepted to&nbsp;<em><font size=4>ICCV 2025</font></em>&nbsp;as an&nbsp;<b style="color:#007bff;"><font size=4>Oral</font></b>&nbsp;paper !!!
                  </li>
                  <li>
                    <span class="news-date">[03/2025]</span> <strong><font size=4>EgoExoLearn</font></strong>&nbsp;won&nbsp;<a href="https://egovis.github.io/awards/2023_2024/" target="_blank"><font size=4>EgoVis 2023/2024 Distinguished Paper Award</font></a>&nbsp;!!!
                  </li>
                  <li>
                    <span class="news-date">[01/2025]</span> Three Papers&nbsp;<strong><font size=4>EgoExo-Gen</font></strong>,&nbsp;<strong><font size=4>EgoVideo</font></strong>,&nbsp;<strong><font size=4>CGBench</font></strong>&nbsp;accepted to&nbsp;<em><font size=4>ICLR 2025</font></em>&nbsp;!!!
                  </li>
                  <li>
                    <span class="news-date">[01/2025]</span> Honored to be invited to give a talk on joint egocentric-exocentric video understanding at&nbsp;<a href="https://www.techbeat.net/talk-info?id=940" target="_blank"><font size=4>TechBeat</font></a>
                  </li>
                  <li>
                    <span class="news-date">[05/2024]</span> Our CVPR papers&nbsp;<strong><font size=4>Egoinstructor</font></strong>&nbsp;and&nbsp;<strong><font size=4>EgoExoLearn</font></strong>&nbsp;are accepted to&nbsp;<em><font size=4>1st LPVL Workshop @ CVPR 2024</font></em>
                  </li>
                  <li>
                    <span class="news-date">[04/2024]</span> We rank&nbsp;<strong><font size=4>1st</font></strong>&nbsp;at 4th-COV19D Competition Track 2 and&nbsp;<strong><font size=4>4th</font></strong>&nbsp;at Track 1 @&nbsp;<em><font size=4>CVPR 2024</font></em>
                  </li>
                </ul>
        
                <style>
                  .news-list {
                    list-style: none;
                    padding: 0;
                    margin: 0;
                    font-family: Arial, Helvetica, sans-serif;
                    font-size: 16px;
                    line-height: 1.7;
                  }
                  .news-list li {
                    display: flex;
                    align-items: baseline;
                    padding: 10px 12px;
                    margin-bottom: 8px;
                    background: #fafafa;
                    border-radius: 8px;
                    transition: background 0.2s;
                  }
                  .news-list li:hover {
                    background: #f0f4ff;
                  }
                  .news-date {
                    flex-shrink: 0;
                    width: 90px;
                    color: #999;
                    font-weight: 500;
                  }
                  .news-list a {
                    color: #007bff;
                    text-decoration: none;
                    font-weight: 500;
                  }
                  .news-list a:hover {
                    text-decoration: underline;
                  }
                </style>
              </td>
            </tr>
          </tbody>
        </table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;vertical-align:middle">
                <h2 style="font-family: Arial, Helvetica, sans-serif; font-size: 26px; font-weight: 700; color: #2c3e50; margin-bottom: 10px;">
                  📑 Research
                </h2>
              </td>
            </tr>
          </tbody>
        </table>
        
        <div class="tab">
          <button class="tablinks active" onclick="openTab(event, 'ComputerVision')">Computer Vision</button>
          <button class="tablinks" onclick="openTab(event, 'Medical')">Medical Image Analysis</button>
        </div>
        
        <div id="ComputerVision" class="tabcontent" style="display:block;">
          <!-- 将Computer Vision相关的研究项目放在这里 -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/vinci.png" alt="vinci" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3749513" class="paper-title"> 
                  Vinci: A Real-time Smart Assistant Based on Egocentric Vision-language Model for Portable Devices
                </a>
                <div class="paper-authors">
                  Yifei Huang*, <b>Jilan Xu*</b>, Baoqi Pei*, Lijin Yang*, MingFang Zhang, Yuping He, Guo Chen, et al.
                </div>
                <div class="paper-venue">
                  <em>IMWUT 2025</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/abs/2412.21080" class="tag">arXiv</a>
                  <a href="https://github.com/OpenGVLab/vinci" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  A real-time vision-language system that can assist users with daily tasks, including scene understanding, grounding, summarization, and future planning.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/streamformer.png" alt="streamformer" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2504.20041" class="paper-title"> 
                  Learning Streaming Video Representation via Multitask Training
                </a>
                <div class="paper-authors">
                  Yibin Yan*, <b>Jilan Xu*</b>, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie
                </div>
                <div class="paper-venue">
                  <em>ICCV 2025, <b>Oral</b></em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2504.20041" class="tag">arXiv</a>
                  <a href="https://go2heart.github.io/streamformer" class="tag">project</a>
                  <a href="https://github.com/Go2Heart/StreamFormer/tree/main" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  A streaming video backbone that learns global, temporal, and spatial video features in a unified visual-textual alignment framework.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/xgen_teaser_square.png" alt="xgen" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2504.11732" class="paper-title"> 
                  EgoExo-Gen: Egocentric Video Prediction by Watching Exocentric Videos
                </a>
                <div class="paper-authors">
                  <b>Jilan Xu</b>, Yifei Huang, Baoqi Pei, Junlin Hou, Qingqiu Li, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                </div>
                <div class="paper-venue">
                  <em>ICLR 2025</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2504.11732" class="tag">arXiv</a>
                </div>
                <p class="paper-desc">
                  A cross-view video prediction model that predicts future egocentric video frames by leveraging paired exocentric video and text instructions.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/egovideo.png" alt="egovideo" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2503.00986" class="paper-title"> 
                  Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning
                </a>
                <div class="paper-authors">
                  Baoqi Pei, Yifei Huang, <b>Jilan Xu</b>, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang
                </div>
                <div class="paper-venue">
                  <em>ICLR 2025</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2503.00986" class="tag">arXiv</a>
                </div>
                <p class="paper-desc">
                  An egocentric video-language model that learns fine-grained egocentric video representations by modeling hand-object dynamics.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/cgbench2.png" alt="cgbench" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2412.12075" class="paper-title"> 
                  CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding
                </a>
                <div class="paper-authors">
                  Guo Chen*, Yicheng Liu*, Yifei Huang*, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, Limin Wang
                </div>
                <div class="paper-venue">
                  <em>ICLR 2025</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2412.12075" class="tag">arXiv</a>
                  <a href="https://cg-bench.github.io/leaderboard/" class="tag">project</a>
                  <a href="https://github.com/CG-Bench/CG-Bench" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  A clue-grounded question answering benchmark for long video understanding.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/egoexolearn.png" alt="egoexolearn" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2403.16182" class="paper-title"> 
                  EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World
                </a>
                <div class="paper-authors">
                  Yifei Huang*, Guo Chen*, <b>Jilan Xu*</b>, Mingfang Zhang*, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, Yu Qiao
                </div>
                <div class="paper-venue">
                  <em>CVPR 2024</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2403.16182.pdf" class="tag">arXiv</a>
                  <a href="https://egoexolearn.github.io/" class="tag">project</a>
                  <a href="https://github.com/OpenGVLab/EgoExoLearn" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  A cross-view benchmark dataset that emulates the human demonstration following process, containing recorded egocentric videos guided by exocentric-view demonstration videos.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/egoinstructor.png" alt="egoinstructor" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2401.00789" class="paper-title"> 
                  Retrieval-Augmented Egocentric Video Captioning
                </a>
                <div class="paper-authors">
                  <b>Jilan Xu</b>, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                </div>
                <div class="paper-venue">
                  <em>CVPR 2024</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2401.00789.pdf" class="tag">arXiv</a>
                  <a href="https://jazzcharles.github.io/Egoinstructor" class="tag">project</a>
                  <a href="https://github.com/Jazzcharles/Egoinstructor" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  Given an egocentric video, Egoinstructor automatically retrieves relevant exocentric instructional videos for assisting egocentric video captioning.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/ovsegmentor.png" alt="ovsegmentor" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2301.09121" class="paper-title">
                  Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision
                </a>
                <div class="paper-authors">
                  <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, Weidi Xie
                </div>
                <div class="paper-venue">
                  <em>CVPR 2023</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2301.09121.pdf" class="tag">arXiv</a>
                  <a href="https://jazzcharles.github.io/OVSegmentor" class="tag">project</a>
                  <a href="https://github.com/Jazzcharles/OVSegmentor" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  Training open-vocabulary semantic segmentation models with image-text pairs only, which enables zero-transfer to various segmentation datasets.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/cream.png" alt="cream" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2205.13922" class="paper-title">
                  CREAM: Weakly supervised object localization via class re-activation mapping
                </a>
                <div class="paper-authors">
                  <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Rui-Wei Zhao, Tao Zhang, Xuequan Lu, Shang Gao
                </div>
                <div class="paper-venue">
                  <em>CVPR 2022</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2205.13922.pdf" class="tag">arXiv</a>
                </div>
                <p class="paper-desc">
                  A weakly-supervised object localization model that generates better CAMs via soft-clustering algorithms.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/ovoad.png" alt="ovoad" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/forum?id=PWzB2V2b6R" class="paper-title">
                  Does video-text pretraining help open vocabulary online action detection
                </a>
                <div class="paper-authors">
                  Qingsong Zhao*, Yi Wang*, <b>Jilan Xu*</b>, Yinan He*, Zifan Song, Limin Wang, Yu Qiao, Cairong Zhao
                </div>
                <div class="paper-venue">
                  <em>NeurIPS 2024</em>
                </div>
                <div class="paper-links">
                  <a href="https://openreview.net/forum?id=PWzB2V2b6R" class="tag">arXiv</a>
                </div>
                <p class="paper-desc">
                  A zero-shot online action detector that leverages vision-language models and enables open-world temporal understanding.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/internvideo.png" alt="internvideo" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2212.03191" class="paper-title">
                  InternVideo: General Video Foundation Models via Generative and Discriminative Learning
                </a>
                <div class="paper-authors">
                  Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, <b>Jilan Xu</b>, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao
                </div>
                <div class="paper-venue">
                  <em>Tech report 2022</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2212.03191.pdf" class="tag">arXiv</a>
                  <a href="https://github.com/OpenGVLab/InternVideo" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  A foundation model for video / video-text understanding, achieving SOTA over 30 benchmark datasets.
                </p>
              </td>
            </tr>

            <style>
              .paper-item {
                background: #fff;
                border-radius: 12px;
                box-shadow: 0 2px 6px rgba(0,0,0,0.08);
                transition: transform 0.2s ease, box-shadow 0.2s ease;
              }
              .paper-item:hover {
                transform: translateY(-3px);
                box-shadow: 0 4px 12px rgba(0,0,0,0.12);
              }
              .paper-img {
                width: 260px;
                height: 195px;
                object-fit: cover;
                border-radius: 8px;
              }
              .paper-title {
                font-size: 20px;
                font-weight: 600;
                color: #2c3e50;
                text-decoration: none;
              }
              .paper-title:hover {
                color: #007bff;
              }
              .paper-authors {
                margin-top: 6px;
                font-size: 15px;
                color: #444;
              }
              .paper-venue {
                margin-top: 4px;
                font-size: 15px;
                color: #555;
              }
              .paper-links {
                margin-top: 6px;
              }
              .tag {
                display: inline-block;
                padding: 3px 8px;
                margin-right: 6px;
                font-size: 14px;
                color: #fff;
                background: #007bff;
                border-radius: 6px;
                text-decoration: none;
                transition: background 0.2s;
              }
              .tag:hover {
                background: #0056b3;
              }
              .paper-desc {
                margin-top: 10px;
                font-size: 15px;
                color: #444;
                line-height: 1.6;
              }
            </style>
            
          </tbody></table>
        </div>
        
        <div id="Medical" class="tabcontent">
          <!-- 将Medical相关的研究项目放在这里 -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- 这里放置Medical相关的项目 -->
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/qmix.png" alt="qmix" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2404.05169" class="paper-title">
                  QMix: Quality-aware Learning with Mixed Noise for Robust Retinal Disease Diagnosis
                </a>
                <div class="paper-authors">
                  Junlin Hou, <b>Jilan Xu</b>, Rui Feng, Hao Chen
                </div>
                <div class="paper-venue">
                  <em>IEEE Transactions on Medical Imaging 2025</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2404.05169" class="tag">arXiv</a>
                </div>
                <p class="paper-desc">
                  A noise learning framework that learns a robust disease diagnosis model under mixed noise scenarios.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/caw.png" alt="caw" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2404.05997" class="paper-title">
                  Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis
                </a>
                <div class="paper-authors">
                  Junlin Hou, <b>Jilan Xu</b>, Hao Chen
                </div>
                <div class="paper-venue">
                  <em>MICCAI 2024</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2404.05997" class="tag">arXiv</a>
                </div>
                <p class="paper-desc">
                  An XAI framework that aligns the axes of the latent space with concepts of interest for interpretable skin lesion diagnosis.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/anatomy.png" alt="anatomy" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2403.09294" class="paper-title">
                  Anatomical structure-guided medical vision-language pre-training
                </a>
                <div class="paper-authors">
                  Qingqiu Li, Xiaohan Yan, <b>Jilan Xu</b>, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Shujun Wang
                </div>
                <div class="paper-venue">
                  <em>MICCAI 2024</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2403.09294" class="tag">arXiv</a>
                </div>
                <p class="paper-desc">
                  An Anatomical Structure-Guided visual-text pre-training framework that leverages the anatomical knowledge.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/cmcv2.png" alt="cmcv2" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2211.14557" class="paper-title">
                  CMC_v2: Towards More Accurate COVID-19 Detection with Discriminative Video Priors
                </a>
                <div class="paper-authors">
                  Junlin Hou, <b>Jilan Xu</b>, Nan Zhang, Yi Wang, Yuejie Zhang, Xiaobo Zhang, Rui Feng
                </div>
                <div class="paper-venue">
                  <em>ECCV 2022 AIMIA Workshop</em>
                </div>
                <div class="paper-links">
                  <a href="https://arxiv.org/pdf/2211.14557.pdf" class="tag">arXiv</a>
                  <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  A Transformer-based model with contrastive representation enhancement. Winner of the 2nd COVID-19 Detection in ECCV 2022.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/tccnet.png" alt="tccnet" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.ijcai.org/proceedings/2022/0155.pdf" class="paper-title">
                  TCCNet: Temporally Consistent Context-Free Network for Semi-supervised Video Polyp Segmentation
                </a>
                <div class="paper-authors">
                  Xiaotong Li, <b>Jilan Xu</b>, Yuejie Zhang, Rui Feng, Rui-Wei Zhao, Tao Zhang, Xuequan Lu, Shang Gao
                </div>
                <div class="paper-venue">
                  <em>IJCAI 2022, Oral</em>
                </div>
                <div class="paper-links">
                  <a href="https://www.ijcai.org/proceedings/2022/0155.pdf" class="tag">paper</a>
                </div>
                <p class="paper-desc">
                  Co-training a model for semi-supervised video polyp segmentation, achieving comparable results using only 15% labeled data.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/cmcv1.png" alt="cmcv1" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf" class="paper-title">
                  CMC-COV19D: Contrastive Mixup Classification for COVID-19 Diagnosis
                </a>
                <div class="paper-authors">
                  Junlin Hou*, <b>Jilan Xu*</b>, Rui Feng, Yuejie Zhang, Fei Shan, Weiya Shi
                </div>
                <div class="paper-venue">
                  <em>ICCV 2021, AIMIA Workshop</em>
                </div>
                <div class="paper-links">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf" class="tag">paper</a>
                  <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution" class="tag">code</a>
                </div>
                <p class="paper-desc">
                  A ResNest-50 model combined with contrastive mixup technique for 3D COVID-19 CT image classification. Winner of the 1st COVID-19 detection challenge.
                </p>
              </td>
            </tr>
            
            <tr class="paper-item">
              <td style="padding:20px;width:10%;vertical-align:middle;">
                <img src="images/drl.png" alt="drl" class="paper-img">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9313159/#:~:text=Data-Efficient%20Histopathology%20Image%20Analysis%20with%20Deformation%20Representation%20Learning,are%20often%20expensive%20and%20laborious%20in%20realworld%20scenarios." class="paper-title">
                  Data-Efficient Histopathology Image Analysis with Deformation Representation Learning
                </a>
                <div class="paper-authors">
                  <b>Jilan Xu</b>, Junlin Hou, Yuejie Zhang, Rui Feng, Chunyang Ruan, Tao Zhang, Weiguo Fan
                </div>
                <div class="paper-venue">
                  <em>BIBM 2020, Oral</em>
                </div>
                <div class="paper-links">
                  <a href="https://ieeexplore.ieee.org/document/9313159/#:~:text=Data-Efficient%20Histopathology%20Image%20Analysis%20with%20Deformation%20Representation%20Learning,are%20often%20expensive%20and%20laborious%20in%20realworld%20scenarios." class="tag">paper</a>
                </div>
                <p class="paper-desc">
                  Introducing a self-supervised deformation representation learning technique for histopathology image analysis.
                </p>
              </td>
            </tr>
            
            <style>
              .paper-item {
                background: #fff;
                border-radius: 12px;
                box-shadow: 0 2px 6px rgba(0,0,0,0.08);
                transition: transform 0.2s ease, box-shadow 0.2s ease;
              }
              .paper-item:hover {
                transform: translateY(-3px);
                box-shadow: 0 4px 12px rgba(0,0,0,0.12);
              }
              .paper-img {
                width: 260px;
                height: 195px;
                object-fit: cover;
                border-radius: 8px;
              }
              .paper-title {
                font-size: 20px;
                font-weight: 600;
                color: #2c3e50;
                text-decoration: none;
              }
              .paper-title:hover {
                color: #007bff;
              }
              .paper-authors {
                margin-top: 6px;
                font-size: 15px;
                color: #444;
              }
              .paper-venue {
                margin-top: 4px;
                font-size: 15px;
                color: #555;
              }
              .paper-links {
                margin-top: 6px;
              }
              .tag {
                display: inline-block;
                padding: 3px 8px;
                margin-right: 6px;
                font-size: 14px;
                color: #fff;
                background: #007bff;
                border-radius: 6px;
                text-decoration: none;
                transition: background 0.2s;
              }
              .tag:hover {
                background: #0056b3;
              }
              .paper-desc {
                margin-top: 10px;
                font-size: 15px;
                color: #444;
                line-height: 1.6;
              }
            </style>
            
          </tbody></table>
        </div>
        
        <style>
          @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');
        
          img {
            border-radius: 15px;
          }
          .tab {
            overflow: hidden;
            background-color: #f8f8f8;
            font-family: 'Roboto', sans-serif;
          }
          .tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 18px;
            font-weight: bold;
            color: #333;
          }
          .tab button:hover {
            background-color: #ddd;
          }
          .tab button.active {
            background-color: #fff;
            border-bottom: 3px solid #4CAF50;
          }
          .tabcontent {
            display: none;
            padding: 20px 0;
            border: none;
          }
          .tabcontent table {
            border: none;
          }
        </style>
        
        <script>
        function openTab(evt, tabName) {
          var i, tabcontent, tablinks;
          tabcontent = document.getElementsByClassName("tabcontent");
          for (i = 0; i < tabcontent.length; i++) {
            tabcontent[i].style.display = "none";
          }
          tablinks = document.getElementsByClassName("tablinks");
          for (i = 0; i < tablinks.length; i++) {
            tablinks[i].className = tablinks[i].className.replace(" active", "");
          }
          document.getElementById(tabName).style.display = "block";
          evt.currentTarget.className += " active";
        }
        </script>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              
        </tbody>
      </table>

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tbody>
            <tr>
              <td>
                <h2 style="font-family: Arial, Helvetica, sans-serif; font-size: 22px; font-weight: 600; margin-bottom: 15px; color: #333;">
                  🏆 Awards & Honors
                </h2>
                <ul style="font-family: Arial, Helvetica, sans-serif; font-size: 16px; line-height: 1.8; color: #444; margin: 0; padding-left: 0; list-style: none;">
                  <li style="padding: 10px 15px; border-bottom: 1px solid #ddd; transition: background 0.3s;">
                    <b>Winner</b> of the <i>6th-COV19D Competition</i> Track 1 (Fair Disease Diagnosis Challenge) and Track 2 (Multi-Source COVID-19 Detection Challenge) @ <b>ICCV 2025</b>
                  </li>
                  <li style="padding: 10px 15px; border-bottom: 1px solid #ddd; transition: background 0.3s;">
                    <b>Winner of the 7 tracks</b> (Natural Language Queries, Visual Queries 2D, Short-term Object Interaction Anticipation, Long-term Action Anticipation, Body Pose, Domain Adaptation for Action Recognition) in the <i>1st EgoVis Workshop</i> @ <b>CVPR 2024</b>
                  </li>
                  <li style="padding: 10px 15px; border-bottom: 1px solid #ddd; transition: background 0.3s;">
                    <b>Winner</b> of the <i>4th-COV19D Competition</i> Track 2 (COVID19 Domain Adaptation Challenge) and <b>4th place</b> at Track 1 (COVID-19 Detection Challenge) @ <b>CVPR 2024</b>
                  </li>
                  <li style="padding: 10px 15px; border-bottom: 1px solid #ddd; transition: background 0.3s;">
                    <b>Winner</b> of the <i>MMAC Challenge</i> Track 1 (Classification of Myopic Maculopathy) and Track 2 (Segmentation of Myopic Maculopathy Plus Lesions) @ <b>MICCAI 2023</b>
                  </li>
                  <li style="padding: 10px 15px; border-bottom: 1px solid #ddd; transition: background 0.3s;">
                    <b>Winner</b> of the <i>1st & 2nd COVID-19 Detection Challenge</i> @ <b>ICCV 2021 & ECCV 2022</b>
                  </li>
                  <li style="padding: 10px 15px; border-bottom: 1px solid #ddd; transition: background 0.3s;">
                    <b>Winner</b> of the <i>1st COVID-19 Severity Detection Challenge</i> @ <b>ECCV 2022</b>
                  </li>
                  <li style="padding: 10px 15px; transition: background 0.3s;">
                    <b>VenusTech Enterprise Scholarship</b>
                  </li>
                </ul>
                <style>
                  ul li:hover {
                    background: #f5f7fa;
                  }
                </style>
              </td>
            </tr>
          </tbody>
        </table>
        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <h2 style="font-family: Arial, Helvetica, sans-serif; font-size: 22px; font-weight: 600; margin-bottom: 20px; color: #333;">
                  💼 Working Experience
                </h2>

                <div class="experience-container">

                  <div class="experience-item">
                    <img src="images/shanghai-ai-lab-logo.png" class="company-logo" alt="Shanghai AI Laboratory Logo">
                    <div class="experience-content">
                      <div class="company">Shanghai AI Laboratory</div>
                      <div class="role">Research Intern</div>
                      <div class="supervisor">Supervised by Dr. Yifei Huang, Yi Wang and Prof. Yu Qiao</div>
                    </div>
                  </div>

                  <div class="experience-item">
                    <img src="images/bell-ai-logo.png" class="company-logo" alt="Bell AI Lab Logo">
                    <div class="experience-content">
                      <div class="company">Bell AI Lab, Shanghai</div> 
                      <div class="role">Research Intern</div>
                      <div class="supervisor">Supervised by Dr. Chenhui Ye</div>
                    </div>
                  </div>

                  <div class="experience-item">
                    <img src="images/google-logo.png" class="company-logo" alt="Google Logo">
                    <div class="experience-content">
                      <div class="company">Google Winter AI Camp</div>
                      <div class="achievement">🏆 Best Presentation Award</div>
                    </div>
                  </div>

                  <div class="experience-item">
                    <img src="images/morgan-stanley-logo.png" class="company-logo" alt="Morgan Stanley Logo">
                    <div class="experience-content">
                      <div class="company">Morgan Stanley Technology</div>
                      <div class="role">Software Engineering Intern</div>
                      <div class="supervisor">Supervised by Ray Zhou</div>
                    </div>
                  </div>

                </div>

                <style>
                  .experience-container {
                    font-size: 16px;
                    line-height: 1.6;
                    display: grid;
                    grid-template-columns: 1fr;
                    gap: 20px;
                  }
                  .experience-item {
                    display: flex;
                    align-items: center;
                    gap: 20px;
                    padding: 15px 20px;
                    background: #fff;
                    border-radius: 12px;
                    box-shadow: 0 2px 6px rgba(0,0,0,0.08);
                    transition: transform 0.2s ease, box-shadow 0.2s ease;
                  }
                  .experience-item:hover {
                    transform: translateY(-3px);
                    box-shadow: 0 4px 12px rgba(0,0,0,0.12);
                  }
                  .company-logo {
                    width: 70px;
                    height: 70px;
                    object-fit: contain;
                    flex-shrink: 0;
                  }
                  .experience-content {
                    flex-grow: 1;
                  }
                  .company {
                    font-weight: bold;
                    font-size: 18px;
                    color: #2c3e50;
                  }
                  .role {
                    color: #007bff;
                    margin-top: 4px;
                    font-size: 15px;
                  }
                  .supervisor {
                    color: #666;
                    font-style: italic;
                    margin-top: 4px;
                    font-size: 14px;
                  }
                  .achievement {
                    color: #28a745;
                    font-weight: 500;
                    margin-top: 4px;
                    font-size: 15px;
                  }
                </style>
              </td>
            </tr>
          </tbody>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <h2 style="font-family: Arial, Helvetica, sans-serif; font-size: 22px; font-weight: 600; margin-bottom: 20px; color: #333;">
                  🎓 Academic Services
                </h2>
        
                <div class="service-section">
                  <div class="service-title">Conference Reviewer</div>
                  <div class="service-content">
                    ICLR25, NeurIPS25/24/22, ECCV24, MICCAI25/24, CVPR24/23, ICCV25/23, ACMMM25, ICML25
                  </div>
                </div>
        
                <div class="service-section">
                  <div class="service-title">Journal Reviewer</div>
                  <div class="service-content">
                    Nature Communications, TPAMI, IJCV, TMM, NeuroComputing
                  </div>
                </div>
        
                <div class="service-section">
                  <div class="service-title">Teaching Assistant (TA)</div>
                  <div class="service-content">
                    Data Structure, The Theory of Computation
                  </div>
                </div>
        
                <style>
                  .service-section {
                    margin-bottom: 15px;
                    padding: 12px 15px;
                    background: #f9fafc;
                    border-left: 4px solid #007bff;
                    border-radius: 6px;
                    transition: background 0.3s;
                  }
                  .service-section:hover {
                    background: #f0f4ff;
                  }
                  .service-title {
                    font-weight: bold;
                    font-size: 16px;
                    color: #2c3e50;
                    margin-bottom: 5px;
                  }
                  .service-content {
                    font-size: 15px;
                    color: #444;
                    line-height: 1.6;
                  }
                </style>
              </td>
            </tr>
          </tbody>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
